{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperasの練習\n",
    "\n",
    "*20170715 Mitsuhiro Ito*\n",
    "\n",
    "\n",
    "* (https://github.com/maxpumperla/hyperas/tree/master/examples)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from utils import save_model_viz, save_weights, save_hist, plot_hist\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import uniform, choice, conditional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    train_gan = ImageDataGenerator(rescale=1.0 / 255,\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    \n",
    "    valid_gan = ImageDataGenerator(rescale=1.0 / 255,\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    train_generator = train_gan.flow_from_directory('data/train/',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)\n",
    "\n",
    "    valid_generator = valid_gan.flow_from_directory('data/valid/',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(train_generator, valid_generator):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D({{choice([32, 64, 128])}}, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D({{choice([32, 64, 128])}}, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    batch_size = 16\n",
    "\n",
    "\n",
    "    model.fit_generator(train_generator, \n",
    "                    steps_per_epoch = 2000 // batch_size, \n",
    "                    epochs=20, \n",
    "                    verbose=1, \n",
    "                    validation_data = valid_generator,\n",
    "                    validation_steps= 800 // batch_size)\n",
    "    \n",
    "    \n",
    "    score, acc = model.evaluate_generator(generator=valid_generator, \n",
    "                                      steps= 800 // batch_size)\n",
    "    \n",
    "    print(score, acc)\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils import save_model_viz, save_weights, save_hist, plot_hist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform, choice, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, Dropout, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Conv2D': hp.choice('Conv2D', [32, 64, 128]),\n",
      "        'Conv2D_1': hp.choice('Conv2D_1', [32, 64, 128]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: train_gan = ImageDataGenerator(rescale=1.0 / 255,\n",
      "   3:     featurewise_center=False,  # set input mean to 0 over the dataset\n",
      "   4:     samplewise_center=False,  # set each sample mean to 0\n",
      "   5:     featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
      "   6:     samplewise_std_normalization=False,  # divide each input by its std\n",
      "   7:     zca_whitening=False,  # apply ZCA whitening\n",
      "   8:     rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
      "   9:     width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
      "  10:     height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
      "  11:     horizontal_flip=False,  # randomly flip images\n",
      "  12:     vertical_flip=False)  # randomly flip images\n",
      "  13: \n",
      "  14: valid_gan = ImageDataGenerator(rescale=1.0 / 255,\n",
      "  15:     featurewise_center=False,  # set input mean to 0 over the dataset\n",
      "  16:     samplewise_center=False,  # set each sample mean to 0\n",
      "  17:     featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
      "  18:     samplewise_std_normalization=False,  # divide each input by its std\n",
      "  19:     zca_whitening=False,  # apply ZCA whitening\n",
      "  20:     rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
      "  21:     width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
      "  22:     height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
      "  23:     horizontal_flip=False,  # randomly flip images\n",
      "  24:     vertical_flip=False)  # randomly flip images\n",
      "  25: \n",
      "  26: train_generator = train_gan.flow_from_directory('data/train/',\n",
      "  27:     target_size=(150, 150),\n",
      "  28:     batch_size=16,\n",
      "  29:     class_mode='binary',\n",
      "  30:     shuffle=False)\n",
      "  31: \n",
      "  32: valid_generator = valid_gan.flow_from_directory('data/valid/',\n",
      "  33:     target_size=(150, 150),\n",
      "  34:     batch_size=16,\n",
      "  35:     class_mode='binary',\n",
      "  36:     shuffle=False)\n",
      "  37: \n",
      "  38: \n",
      "  39: \n",
      "  40: \n",
      "  41: \n",
      "  42: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     model = Sequential()\n",
      "   5:     model.add(Conv2D(space['Conv2D'], (3, 3), input_shape=(150, 150, 3)))\n",
      "   6:     model.add(Activation('relu'))\n",
      "   7:     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "   8: \n",
      "   9:     model.add(Conv2D(space['Conv2D_1'], (3, 3)))\n",
      "  10:     model.add(Activation('relu'))\n",
      "  11:     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "  12: \n",
      "  13:     model.add(Conv2D(64, (3, 3)))\n",
      "  14:     model.add(Activation('relu'))\n",
      "  15:     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "  16:     \n",
      "  17:     model.add(Flatten())\n",
      "  18:     model.add(Dense(64))\n",
      "  19:     model.add(Activation('relu'))\n",
      "  20:     model.add(Dropout(space['Dropout']))\n",
      "  21:     model.add(Dense(1))\n",
      "  22:     model.add(Activation('sigmoid'))\n",
      "  23: \n",
      "  24:     model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
      "  25:     \n",
      "  26:     batch_size = 16\n",
      "  27: \n",
      "  28: \n",
      "  29:     model.fit_generator(train_generator, \n",
      "  30:                     steps_per_epoch = 2000 // batch_size, \n",
      "  31:                     epochs=20, \n",
      "  32:                     verbose=1, \n",
      "  33:                     validation_data = valid_generator,\n",
      "  34:                     validation_steps= 800 // batch_size)\n",
      "  35:     \n",
      "  36:     \n",
      "  37:     score, acc = model.evaluate_generator(generator=valid_generator, \n",
      "  38:                                       steps= 800 // batch_size)\n",
      "  39:     \n",
      "  40:     print(score, acc)\n",
      "  41:     \n",
      "  42:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  43: \n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 190s - loss: 8.0646 - acc: 0.4980 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 187s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 187s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 187s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 187s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 185s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 187s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 185s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 184s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 185s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 184s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 185s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 183s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 185s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 184s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 184s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 185s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 186s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 186s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 184s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "8.05904774897 0.5\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 83s - loss: 8.0657 - acc: 0.4945 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 83s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 83s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 83s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 85s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 84s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "8.05904774897 0.5\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 127s - loss: 8.0647 - acc: 0.4975 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 126s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 126s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 126s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 128s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 128s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 128s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 128s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 127s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 126s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 126s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 126s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "8.05904774897 0.5\n",
      "Evalutation of best performing model:\n",
      "[6.2961310757614228, 0.609375]\n",
      "{'Dropout': 0.4371162594318422, 'Conv2D_1': 0, 'Conv2D': 2}\n"
     ]
    }
   ],
   "source": [
    "train_generator, valid_generator = data()\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=3,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='20170714_hyperas4' )\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate_generator(valid_generator, steps=64))\n",
    "print(best_run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
