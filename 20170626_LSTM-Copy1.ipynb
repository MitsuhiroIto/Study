{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMのお勉強\n",
    "\n",
    "*20170621６　Mitsuhiro Ito*\n",
    "\n",
    "自分で書いてみよう！  \n",
    "目標　LSTMを利用して夏目漱石っぽい文章を書いてみよう\n",
    "\n",
    "* [MXnetのRNNの公式チュートリアル](http://mxnet.io/tutorials/r/charRnnModel.html)\n",
    "* [keras公式ドキュメント](http://tjo.hatenablog.com/entry/2016/11/08/190000)\n",
    "* [@aidiaryさんの記事](http://qiita.com/elm200/items/6f84d3a42eebe6c47caa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 文章ファイルの入手"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記サイトから上の記事で共有してくれている夏目漱石のテキストをダウンロード  \n",
    "https://github.com/elm200/ltsm_soseki/tree/master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 6578167\n",
      "さんじやまみちをのぼりながら、こうかんがえた。\n",
      "さとしちにはたらけばかくかどがたつ。じょうじょうにさおさおさせばながされる。いじをつうとおせばきゅうくつきゅうくつだ。とかくにひとのよはすみにくい。\n",
      "すみにくさがこうこうじると、やすいところへひきこしたくなる。どこへこし�\n"
     ]
    }
   ],
   "source": [
    "path = \"souseki_all_hiragana2.txt\"\n",
    "text = open(path).read().lower() #lowerは参考元が英語だったため残っているだけで、ひらがな文章では必要ない？\n",
    "print('corpus length:', len(text)) \n",
    "print(text[0:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. 文章ファイルをデータ化\n",
    "\n",
    "まずはdict化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb chars: 66\n",
      "{'\\x81': 2, '\\x80': 1, '\\x83': 4, '\\x82': 3, '\\x85': 6, '\\x84': 5, '\\x87': 8, '\\x86': 7, '\\x89': 10, '\\x88': 9, '\\x8b': 12, '\\n': 0, '\\x8d': 14, '\\x8c': 13, '\\x8f': 16, '\\x8e': 15, '\\x91': 18, '\\x90': 17, '\\x93': 20, '\\x92': 19, '\\x95': 22, '\\x94': 21, '\\x97': 24, '\\x96': 23, '\\x99': 26, '\\x98': 25, '\\x9b': 28, '\\x9a': 27, '\\x9d': 30, '\\x9c': 29, '\\x9f': 32, '\\x9e': 31, '\\xa1': 34, '\\xa0': 33, '\\xa3': 36, '\\xa2': 35, '\\xa5': 38, '\\xa4': 37, '\\xa7': 40, '\\xa6': 39, '\\xa9': 42, '\\xa8': 41, '\\xab': 44, '\\xaa': 43, '\\xad': 46, '\\xac': 45, '\\xaf': 48, '\\xae': 47, '\\xb1': 50, '\\xb0': 49, '\\xb3': 52, '\\xb2': 51, '\\xb5': 54, '\\xb4': 53, '\\xb7': 56, '\\xb6': 55, '\\xb9': 58, '\\xb8': 57, '\\xbb': 60, '\\xba': 59, '\\xbd': 62, '\\xbc': 61, '\\xbf': 64, '\\xbe': 63, '\\x8a': 11, '\\xe3': 65}\n",
      "{0: '\\n', 1: '\\x80', 2: '\\x81', 3: '\\x82', 4: '\\x83', 5: '\\x84', 6: '\\x85', 7: '\\x86', 8: '\\x87', 9: '\\x88', 10: '\\x89', 11: '\\x8a', 12: '\\x8b', 13: '\\x8c', 14: '\\x8d', 15: '\\x8e', 16: '\\x8f', 17: '\\x90', 18: '\\x91', 19: '\\x92', 20: '\\x93', 21: '\\x94', 22: '\\x95', 23: '\\x96', 24: '\\x97', 25: '\\x98', 26: '\\x99', 27: '\\x9a', 28: '\\x9b', 29: '\\x9c', 30: '\\x9d', 31: '\\x9e', 32: '\\x9f', 33: '\\xa0', 34: '\\xa1', 35: '\\xa2', 36: '\\xa3', 37: '\\xa4', 38: '\\xa5', 39: '\\xa6', 40: '\\xa7', 41: '\\xa8', 42: '\\xa9', 43: '\\xaa', 44: '\\xab', 45: '\\xac', 46: '\\xad', 47: '\\xae', 48: '\\xaf', 49: '\\xb0', 50: '\\xb1', 51: '\\xb2', 52: '\\xb3', 53: '\\xb4', 54: '\\xb5', 55: '\\xb6', 56: '\\xb7', 57: '\\xb8', 58: '\\xb9', 59: '\\xba', 60: '\\xbb', 61: '\\xbc', 62: '\\xbd', 63: '\\xbe', 64: '\\xbf', 65: '\\xe3'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text))) # set :重複しない順序なしコレクション\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))#  文字　：　番号\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))#　　番号：　文字\n",
    "print('nb chars:', len(chars))\n",
    "\n",
    "print (char_indices)\n",
    "print (indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ひらがなはタプルやセットに入れるとUTF-8コードに変わる   \n",
    "``` $ s = ('あ','い','う','え','お','か')\n",
    "$ s\n",
    "('\\xe3\\x81\\x82',\n",
    " '\\xe3\\x81\\x84',\n",
    " '\\xe3\\x81\\x86',\n",
    " '\\xe3\\x81\\x88',\n",
    " '\\xe3\\x81\\x8a',\n",
    " '\\xe3\\x81\\x8b')``` \n",
    " \n",
    " ---\n",
    " \n",
    " 次に学習用に４０個の連続した文字のかたまりとその次の文字のデータを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 2192709\n",
      "nb next_chars: 2192709\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40 \n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print('nb next_chars:', len(next_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```$ print(tuple(text[0:41]))```　最初の４１文字から...  \n",
    "('\\xe3', '\\x81', '\\x95', '\\xe3', '\\x82', '\\x93', '\\xe3', '\\x81', '\\x98', '\\xe3', '\\x82', '\\x84', '\\xe3', '\\x81', '\\xbe', '\\xe3', '\\x81', '\\xbf', '\\xe3', '\\x81', '\\xa1', '\\xe3', '\\x82', '\\x92', '\\xe3', '\\x81', '\\xae', '\\xe3', '\\x81', '\\xbc', '\\xe3', '\\x82', '\\x8a', '\\xe3', '\\x81', '\\xaa', '\\xe3', '\\x81', '\\x8c', '\\xe3', '\\x82')   \n",
    "\n",
    "```$　print(sentences[0:1])```   sentenceに最初の４０文字を格納\n",
    "['\\xe3\\x81\\x95\\xe3\\x82\\x93\\xe3\\x81\\x98\\xe3\\x82\\x84\\xe3\\x81\\xbe\\xe3\\x81\\xbf\\xe3\\x81\\xa1\\xe3\\x82\\x92\\xe3\\x81\\xae\\xe3\\x81\\xbc\\xe3\\x82\\x8a\\xe3\\x81\\xaa\\xe3\\x81\\x8c\\xe3']\n",
    "\n",
    "```$　print(next_chars[0:1])```   next_charsに次の文字（４１番目の文字）を格納  \n",
    "['\\x82']\n",
    "\n",
    "文字３個分ずらして格納していく\n",
    "\n",
    "---\n",
    "\n",
    "次に機械学習ができるように行列化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "#  sentenceのリストサイズ　×　 1sentenceの文字数　×　　文字の種類　　のゼロ行列 \n",
    "# 0:False 1: True\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)# sentenceのリストサイズ　　×　　文字の種類 のゼロ行列　  \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2192709, 40, 66), (2192709, 66))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Xについて \n",
    "\n",
    "```$　print (char_indices)```   \n",
    "{'\\x81': 2, '\\x80': 1, '\\x83': 4, '\\x82': 3, '\\x85': 6, '\\x84': 5, '\\x87': 8, '\\x86': 7, '\\x89': 10, '\\x88': 9, '\\x8b': 12, '\\n': 0, '\\x8d': 14, '\\x8c': 13, '\\x8f': 16, '\\x8e': 15, '\\x91': 18, '\\x90': 17, '\\x93': 20, '\\x92': 19, '\\x95': 22, '\\x94': 21, '\\x97': 24, '\\x96': 23, '\\x99': 26, '\\x98': 25, '\\x9b': 28, '\\x9a': 27, '\\x9d': 30, '\\x9c': 29, '\\x9f': 32, '\\x9e': 31, '\\xa1': 34, '\\xa0': 33, '\\xa3': 36, '\\xa2': 35, '\\xa5': 38, '\\xa4': 37, '\\xa7': 40, '\\xa6': 39, '\\xa9': 42, '\\xa8': 41, '\\xab': 44, '\\xaa': 43, '\\xad': 46, '\\xac': 45, '\\xaf': 48, '\\xae': 47, '\\xb1': 50, '\\xb0': 49, '\\xb3': 52, '\\xb2': 51, '\\xb5': 54, '\\xb4': 53, '\\xb7': 56, '\\xb6': 55, '\\xb9': 58, '\\xb8': 57, '\\xbb': 60, '\\xba': 59, '\\xbd': 62, '\\xbc': 61, '\\xbf': 64, '\\xbe': 63, '\\x8a': 11, '\\xe3': 65}　　\n",
    "\n",
    "```$　print(sentences[0:2])``` \n",
    "['\\xe3\\x81\\x95\\xe3\\x82\\x93\\xe3\\x81\\x98\\xe3\\x82\\x84\\xe3\\x81\\xbe\\xe3\\x81\\xbf\\xe3\\x81\\xa1\\xe3\\x82\\x92\\xe3\\x81\\xae\\xe3\\x81\\xbc\\xe3\\x82\\x8a\\xe3\\x81\\xaa\\xe3\\x81\\x8c\\xe3', '\\xe3\\x82\\x93\\xe3\\x81\\x98\\xe3\\x82\\x84\\xe3\\x81\\xbe\\xe3\\x81\\xbf\\xe3\\x81\\xa1\\xe3\\x82\\x92\\xe3\\x81\\xae\\xe3\\x81\\xbc\\xe3\\x82\\x8a\\xe3\\x81\\xaa\\xe3\\x81\\x8c\\xe3\\x82\\x89\\xe3']\n",
    "\n",
    "上の値からわかるように  \n",
    "i =0、t=0、char=sentence[0]の0番目 \\xe3      char_indices[\\xe3]=65   \n",
    "i =0、t=1、char=sentence[0]の1番目 \\x81      char_indices[\\x81]=2   \n",
    "i =0、t=2、char=sentence[0]の2番目 \\x95       char_indices[\\x95]=22   \n",
    "...   \n",
    "i =0、t=39、char=sentence[0]の39番目 \\xe3   char_indices[\\x95]=65\n",
    "       \n",
    "i =1、t=0、char=sentence[1]の0番目 \\xe3    char_indices[\\xe3]=65  \n",
    "i =1、t=1、char=sentence[1]の1番目 \\x82   char_indices[\\x81]=3   \n",
    "i =1、t=2、char=sentence[1]の2番目 \\x93   char_indices[\\x93]=20   \n",
    "...   \n",
    "i =1、t=39、char=sentence[1]の39番目 \\xe3   char_indices[\\x95]=65\n",
    "\n",
    "$ print(X[0,0,65],X[0,1,2],X[0,2,22],X[1,0,65],X[1,1,3],X[1,2,20])   \n",
    "True True True True True True\n",
    "\n",
    "よって　　X =   \n",
    "[[0,0,0,.......,1],[0,0,1,0,....,0],[0,0,0,.. 0,1,0..,0],....[0,0,0,......,1],   \n",
    "[0,0,0,.......,1],[0,0,0,1,....,0],[0,0,0,.. 0,1,0..,0],....[0,0,0,......,1],..   \n",
    ".....]]\n",
    "              \n",
    "\n",
    "つまり４０文字の集合がonehot表現で2192709個集まったもの\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yについて \n",
    "\n",
    "```$ print(next_chars[0:10])```   \n",
    "['\\x82', '\\x80', '\\x81', '\\x81', '\\x81', '\\x82', '\\x81', '\\x81', '\\x81', '\\x80']　　\n",
    "\n",
    "```$　print (char_indices)```   \n",
    "{'\\x81': 2, '\\x80': 1, '\\x83': 4, '\\x82': 3, '\\x85': 6, '\\x84': 5, '\\x87': 8, '\\x86': 7, '\\x89': 10, '\\x88': 9, '\\x8b': 12, '\\n': 0, '\\x8d': 14, '\\x8c': 13, '\\x8f': 16, '\\x8e': 15, '\\x91': 18, '\\x90': 17, '\\x93': 20, '\\x92': 19, '\\x95': 22, '\\x94': 21, '\\x97': 24, '\\x96': 23, '\\x99': 26, '\\x98': 25, '\\x9b': 28, '\\x9a': 27, '\\x9d': 30, '\\x9c': 29, '\\x9f': 32, '\\x9e': 31, '\\xa1': 34, '\\xa0': 33, '\\xa3': 36, '\\xa2': 35, '\\xa5': 38, '\\xa4': 37, '\\xa7': 40, '\\xa6': 39, '\\xa9': 42, '\\xa8': 41, '\\xab': 44, '\\xaa': 43, '\\xad': 46, '\\xac': 45, '\\xaf': 48, '\\xae': 47, '\\xb1': 50, '\\xb0': 49, '\\xb3': 52, '\\xb2': 51, '\\xb5': 54, '\\xb4': 53, '\\xb7': 56, '\\xb6': 55, '\\xb9': 58, '\\xb8': 57, '\\xbb': 60, '\\xba': 59, '\\xbd': 62, '\\xbc': 61, '\\xbf': 64, '\\xbe': 63, '\\x8a': 11, '\\xe3': 65}　　\n",
    "\n",
    "上の値からわかるように   \n",
    "i =0のとき   \n",
    "next_chars[0]=\\x82   \n",
    "char_indices[next_chars[0]]=char_indices[\\x82]=3   \n",
    "\n",
    "i =1のとき   \n",
    "next_chars[1]=\\x80   \n",
    "char_indices[next_chars[0]]=char_indices[\\x82]=1 \n",
    "\n",
    " $ print(y[0,3],y[1,1])   \n",
    " True  True\n",
    "\n",
    "よって　　y = [[0,0,0,1,0,0,.......,0],[0,1,0....,0],....]  \n",
    "つまり次の文字の集合がonehot表現で2192709個集まったもの\n",
    "          \n",
    "\n",
    " \n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ４. LSTMを用いた学習と予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))#１２８：ユニット数\n",
    "model.add(Dense(len(chars)))#ユニット数は文字の種類だけ\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LSTM(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               99840     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 66)                8514      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 66)                0         \n",
      "=================================================================\n",
      "Total params: 108,354\n",
      "Trainable params: 108,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99840\n"
     ]
    }
   ],
   "source": [
    "print(128*65*12) # ユニット数　×　文字の種類　×　（出力、入力、忘却ゲートそれぞれ３個ずつ）？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')#予測した値を行列化\n",
    "    preds = np.log(preds) / temperature　#log表示に\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)#多項分布で確率表現\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, 60):\n",
    "    model.fit(X, y,batch_size=128,epochs=1) \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)# ランダムにスタートのインデックスを決める\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]#ランダムに決めたスタートから４０文字を選び、\n",
    "        generated += sentence# 文章を作成していく\n",
    "        sys.stdout.write(generated)#自動で入るスペースや改行を無視\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))# １×40×66のゼロ行列を作成\n",
    "            for t, char in enumerate(sentence):#上で作ったsentenceを予測に使うように行列化\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]#入力x（sentenceの行列表現）に対して次の文字を予測\n",
    "            next_index = sample(preds, diversity)#予測した文字をindex表現に\n",
    "            next_char = indices_char[next_index]# index表示（onehot）を文字表示に\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Memo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/elm200/ltsm_soseki/blob/master/souseki_all_hiragana2.txt\n",
      " - 0s\n",
      " - 0s\n",
      " - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/mitsuhiro/Work/Study/souseki_all'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file( '/Users/mitsuhiro/Work/Study/souseki_all','https://github.com/elm200/ltsm_soseki/blob/master/souseki_all_hiragana2.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "なぜかうまくいかない..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
